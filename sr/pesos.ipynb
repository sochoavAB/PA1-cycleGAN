{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo contiene un state_dict. Llaves disponibles:\n",
      " - conv1.weight\n",
      " - conv1.bias\n",
      " - conv2.weight\n",
      " - conv2.bias\n",
      " - conv3.weight\n",
      " - conv3.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = '../models/Git/SRCNN.pth'# Cambia esto por la ruta al archivo\n",
    "\n",
    "# Cargar el modelo\n",
    "model_data = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "# Inspeccionar el contenido\n",
    "if isinstance(model_data, dict):\n",
    "    print(\"El archivo contiene un state_dict. Llaves disponibles:\")\n",
    "    for key in model_data.keys():\n",
    "        print(f\" - {key}\")\n",
    "else:\n",
    "    print(\"El archivo contiene un modelo completo o un objeto desconocido:\")\n",
    "    print(type(model_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)  # Primera capa\n",
    "        self.layer2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)  # Segunda capa\n",
    "        self.layer3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)  # Tercera capa\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SRCNN:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([64, 1, 9, 9]) from checkpoint, the shape in current model is torch.Size([64, 3, 9, 9]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([1, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([3, 32, 5, 5]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     11\u001b[0m adjusted_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer1.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv1.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer1.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv1.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer3.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv3.bias\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Cargar los pesos ajustados en el modelo\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43msrcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43madjusted_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Mover al dispositivo y establecer en modo evaluación\u001b[39;00m\n\u001b[0;32m     24\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SRCNN:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([64, 1, 9, 9]) from checkpoint, the shape in current model is torch.Size([64, 3, 9, 9]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([1, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([3, 32, 5, 5]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([3])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Definir el modelo SRCNN\n",
    "srcnn = SRCNN()\n",
    "\n",
    "# Ruta al archivo de pesos\n",
    "model_path = '../models/Git/SRCNN.pth'  # Cambia esto por la ruta al archivo\n",
    "\n",
    "# Cargar los pesos y ajustar los nombres de las claves\n",
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "adjusted_state_dict = {\n",
    "    \"layer1.weight\": state_dict[\"conv1.weight\"],\n",
    "    \"layer1.bias\": state_dict[\"conv1.bias\"],\n",
    "    \"layer2.weight\": state_dict[\"conv2.weight\"],\n",
    "    \"layer2.bias\": state_dict[\"conv2.bias\"],\n",
    "    \"layer3.weight\": state_dict[\"conv3.weight\"],\n",
    "    \"layer3.bias\": state_dict[\"conv3.bias\"],\n",
    "}\n",
    "\n",
    "# Cargar los pesos ajustados en el modelo\n",
    "srcnn.load_state_dict(adjusted_state_dict)\n",
    "\n",
    "# Mover al dispositivo y establecer en modo evaluación\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "srcnn.to(device)\n",
    "srcnn.eval()\n",
    "\n",
    "print(\"Modelo SRCNN cargado exitosamente con los pesos ajustados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)  # Cambiar entrada a 1 canal\n",
    "        self.layer2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.layer3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)  # Salida de 1 canal\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "adjusted_state_dict = {}\n",
    "\n",
    "# Ajustar conv1 (de 1 canal a 3 canales)\n",
    "adjusted_state_dict[\"layer1.weight\"] = state_dict[\"conv1.weight\"].repeat(1, 3, 1, 1) / 3\n",
    "adjusted_state_dict[\"layer1.bias\"] = state_dict[\"conv1.bias\"]\n",
    "\n",
    "# Copiar sin cambios para conv2\n",
    "adjusted_state_dict[\"layer2.weight\"] = state_dict[\"conv2.weight\"]\n",
    "adjusted_state_dict[\"layer2.bias\"] = state_dict[\"conv2.bias\"]\n",
    "\n",
    "# Ajustar conv3 (de 1 canal a 3 canales)\n",
    "adjusted_state_dict[\"layer3.weight\"] = state_dict[\"conv3.weight\"].repeat(3, 1, 1, 1)\n",
    "adjusted_state_dict[\"layer3.bias\"] = state_dict[\"conv3.bias\"].repeat(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo SRCNN cargado exitosamente con los pesos ajustados.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definir la arquitectura SRCNN para imágenes RGB\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)  # Tres canales de entrada\n",
    "        self.layer2 = nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.layer3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)  # Tres canales de salida\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# Cargar pesos ajustados\n",
    "model_path = '../models/Git/SRCNN.pth'\n",
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "# Ajustar los pesos para imágenes RGB\n",
    "adjusted_state_dict = {}\n",
    "adjusted_state_dict[\"layer1.weight\"] = state_dict[\"conv1.weight\"].repeat(1, 3, 1, 1) / 3\n",
    "adjusted_state_dict[\"layer1.bias\"] = state_dict[\"conv1.bias\"]\n",
    "adjusted_state_dict[\"layer2.weight\"] = state_dict[\"conv2.weight\"]\n",
    "adjusted_state_dict[\"layer2.bias\"] = state_dict[\"conv2.bias\"]\n",
    "adjusted_state_dict[\"layer3.weight\"] = state_dict[\"conv3.weight\"].repeat(3, 1, 1, 1)\n",
    "adjusted_state_dict[\"layer3.bias\"] = state_dict[\"conv3.bias\"].repeat(3)\n",
    "\n",
    "# Crear el modelo y cargar los pesos ajustados\n",
    "srcnn = SRCNN()\n",
    "srcnn.load_state_dict(adjusted_state_dict)\n",
    "\n",
    "# Configurar dispositivo y modo evaluación\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "srcnn.to(device)\n",
    "srcnn.eval()\n",
    "\n",
    "print(\"Modelo SRCNN cargado exitosamente con los pesos ajustados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesada muestra 1 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 2 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 3 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 4 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 5 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 6 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 7 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 8 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 9 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 10 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 11 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 12 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 13 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 14 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 15 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 16 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 17 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 18 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 19 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 20 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 21 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 22 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 23 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 24 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 25 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 26 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 27 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 28 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 29 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 30 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 31 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 32 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 33 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 34 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 35 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 36 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 37 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 38 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 39 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 40 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 41 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 42 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 43 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 44 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 45 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 46 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 47 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 48 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 49 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 50 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 51 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 52 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 53 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 54 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 55 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 56 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 57 -> Imágenes guardadas en: ../results/srcnn_outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SRCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.layer2 = torch.nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.layer3 = torch.nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def apply_srcnn(lr_image, model, device):\n",
    "    \"\"\"\n",
    "    Aplica el modelo SRCNN a la imagen de baja resolución.\n",
    "    Args:\n",
    "        lr_image (numpy.ndarray): Imagen de baja resolución\n",
    "        model (torch.nn.Module): Modelo SRCNN\n",
    "        device (torch.device): Dispositivo para ejecutar el modelo\n",
    "    Returns:\n",
    "        numpy.ndarray: Imagen de alta resolución generada\n",
    "    \"\"\"\n",
    "    # Si la imagen tiene un solo canal, expandir a 3 canales\n",
    "    if len(lr_image.shape) == 2 or lr_image.shape[-1] == 1:\n",
    "        lr_image = np.stack([lr_image] * 3, axis=-1)\n",
    "    lr_tensor = ToTensor()(lr_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        sr_tensor = model(lr_tensor)\n",
    "    sr_image = ToPILImage()(sr_tensor.squeeze(0).cpu())\n",
    "    return np.array(sr_image)\n",
    "\n",
    "\n",
    "def process_npz_dataset(dataset_path, output_dir, model, device):\n",
    "    \"\"\"\n",
    "    Procesa todas las imágenes en un archivo .npz usando SRCNN y guarda los resultados.\n",
    "    Args:\n",
    "        dataset_path (str): Ruta al archivo .npz\n",
    "        output_dir (str): Ruta a la carpeta donde se guardarán las imágenes generadas\n",
    "        model (torch.nn.Module): Modelo SRCNN\n",
    "        device (torch.device): Dispositivo para ejecutar el modelo\n",
    "    \"\"\"\n",
    "    # Crear carpeta de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Cargar el archivo .npz\n",
    "    data = np.load(dataset_path)\n",
    "    lr_images = data['arr_0']  # Baja resolución\n",
    "    hr_images = data['arr_1']  # Alta resolución (ground truth)\n",
    "\n",
    "    for i in range(len(lr_images)):\n",
    "        # Obtener las imágenes de baja y alta resolución\n",
    "        lr_image = (lr_images[i].astype(np.float32) / 255.0)  # Normalizar a [0, 1]\n",
    "        hr_image = (hr_images[i].astype(np.float32) / 255.0)\n",
    "\n",
    "        # Asegurar que ambas imágenes sean RGB\n",
    "        if len(lr_image.shape) == 2 or lr_image.shape[-1] == 1:\n",
    "            lr_image = np.stack([lr_image] * 3, axis=-1)\n",
    "        if len(hr_image.shape) == 2 or hr_image.shape[-1] == 1:\n",
    "            hr_image = np.stack([hr_image] * 3, axis=-1)\n",
    "\n",
    "        # Pasar la imagen de baja resolución por el modelo SRCNN\n",
    "        sr_image = apply_srcnn(lr_image, model, device)\n",
    "\n",
    "        # Guardar las imágenes generadas\n",
    "        Image.fromarray((lr_image * 255).astype(np.uint8)).save(os.path.join(output_dir, f\"lr_{i+1}.png\"))\n",
    "        Image.fromarray((hr_image * 255).astype(np.uint8)).save(os.path.join(output_dir, f\"hr_{i+1}.png\"))\n",
    "        Image.fromarray((sr_image * 255).astype(np.uint8)).save(os.path.join(output_dir, f\"sr_{i+1}.png\"))\n",
    "\n",
    "        print(f\"Procesada muestra {i+1} -> Imágenes guardadas en: {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ruta al archivo .npz\n",
    "    dataset_path = \"../data/confocal_exper_paired_filt_validsetR_256.npz\"\n",
    "    # Ruta para guardar las imágenes generadas\n",
    "    output_dir = \"../results/srcnn_outputs\"\n",
    "\n",
    "    # Configurar el dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Definir el modelo SRCNN\n",
    "    srcnn = SRCNN()\n",
    "\n",
    "    # Ajustar los pesos (usa adjusted_state_dict previamente generado)\n",
    "    adjusted_state_dict = {\n",
    "        \"layer1.weight\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv1.weight\"].repeat(1, 3, 1, 1) / 3,\n",
    "        \"layer1.bias\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv1.bias\"],\n",
    "        \"layer2.weight\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv2.weight\"],\n",
    "        \"layer2.bias\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv2.bias\"],\n",
    "        \"layer3.weight\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv3.weight\"].repeat(3, 1, 1, 1),\n",
    "        \"layer3.bias\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv3.bias\"].repeat(3),\n",
    "    }\n",
    "    srcnn.load_state_dict(adjusted_state_dict)\n",
    "\n",
    "    # Mover al dispositivo y configurar en modo evaluación\n",
    "    srcnn.to(device)\n",
    "    srcnn.eval()\n",
    "\n",
    "    # Procesar el dataset\n",
    "    process_npz_dataset(dataset_path, output_dir, srcnn, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arr_0': (57, 256, 256, 3), 'arr_1': (57, 256, 256, 3)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ruta al archivo .npz\n",
    "dataset_path = \"../data/confocal_exper_paired_filt_validsetR_256.npz\"\n",
    "\n",
    "# Cargar el dataset y explorar sus contenidos\n",
    "data = np.load(dataset_path)\n",
    "\n",
    "# Obtener las claves y dimensiones de los datos\n",
    "dataset_info = {key: data[key].shape for key in data.keys()}\n",
    "dataset_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesada muestra 1 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 2 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 3 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 4 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 5 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 6 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 7 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 8 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 9 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 10 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 11 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 12 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 13 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 14 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 15 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 16 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 17 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 18 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 19 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 20 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 21 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 22 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 23 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 24 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 25 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 26 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 27 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 28 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 29 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 30 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 31 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 32 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 33 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 34 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 35 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 36 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 37 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 38 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 39 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 40 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 41 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 42 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 43 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 44 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 45 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 46 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 47 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 48 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 49 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 50 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 51 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 52 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 53 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 54 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 55 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 56 -> Imágenes guardadas en: ../results/srcnn_outputs\n",
      "Procesada muestra 57 -> Imágenes guardadas en: ../results/srcnn_outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SRCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.layer2 = torch.nn.Conv2d(64, 32, kernel_size=5, padding=2)\n",
    "        self.layer3 = torch.nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def apply_srcnn(lr_image, model, device):\n",
    "    \"\"\"\n",
    "    Aplica el modelo SRCNN a la imagen de baja resolución.\n",
    "    Args:\n",
    "        lr_image (numpy.ndarray): Imagen de baja resolución\n",
    "        model (torch.nn.Module): Modelo SRCNN\n",
    "        device (torch.device): Dispositivo para ejecutar el modelo\n",
    "    Returns:\n",
    "        numpy.ndarray: Imagen de alta resolución generada\n",
    "    \"\"\"\n",
    "    lr_tensor = ToTensor()(lr_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        sr_tensor = model(lr_tensor)\n",
    "    sr_image = ToPILImage()(sr_tensor.squeeze(0).cpu())\n",
    "    return np.array(sr_image)\n",
    "\n",
    "\n",
    "def process_npz_dataset(dataset_path, output_dir, model, device):\n",
    "    \"\"\"\n",
    "    Procesa todas las imágenes en un archivo .npz usando SRCNN y guarda los resultados.\n",
    "    Args:\n",
    "        dataset_path (str): Ruta al archivo .npz\n",
    "        output_dir (str): Ruta a la carpeta donde se guardarán las imágenes generadas\n",
    "        model (torch.nn.Module): Modelo SRCNN\n",
    "        device (torch.device): Dispositivo para ejecutar el modelo\n",
    "    \"\"\"\n",
    "    # Crear carpeta de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Cargar el archivo .npz\n",
    "    data = np.load(dataset_path)\n",
    "    lr_images = data['arr_0']  # Baja resolución\n",
    "    hr_images = data['arr_1']  # Alta resolución (ground truth)\n",
    "\n",
    "    for i in range(len(lr_images)):\n",
    "        # Obtener las imágenes de baja y alta resolución\n",
    "        lr_image = (lr_images[i].astype(np.float32) / 255.0)  # Normalizar a [0, 1]\n",
    "        hr_image = (hr_images[i].astype(np.float32) / 255.0)\n",
    "\n",
    "        # Pasar la imagen de baja resolución por el modelo SRCNN\n",
    "        sr_image = apply_srcnn(lr_image, model, device)\n",
    "\n",
    "        # Guardar las imágenes generadas\n",
    "        Image.fromarray((lr_image * 255).astype(np.uint8)).save(os.path.join(output_dir, f\"lr_{i+1}.png\"))\n",
    "        Image.fromarray((hr_image * 255).astype(np.uint8)).save(os.path.join(output_dir, f\"hr_{i+1}.png\"))\n",
    "        Image.fromarray((sr_image * 255).astype(np.uint8)).save(os.path.join(output_dir, f\"sr_{i+1}.png\"))\n",
    "\n",
    "        print(f\"Procesada muestra {i+1} -> Imágenes guardadas en: {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ruta al archivo .npz\n",
    "    dataset_path = \"../data/confocal_exper_paired_filt_validsetR_256.npz\"\n",
    "    # Ruta para guardar las imágenes generadas\n",
    "    output_dir = \"../results/srcnn_outputs\"\n",
    "\n",
    "    # Configurar el dispositivo\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Definir el modelo SRCNN\n",
    "    srcnn = SRCNN()\n",
    "\n",
    "    # Ajustar los pesos (usa adjusted_state_dict previamente generado)\n",
    "    adjusted_state_dict = {\n",
    "        \"layer1.weight\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv1.weight\"].repeat(1, 3, 1, 1) / 3,\n",
    "        \"layer1.bias\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv1.bias\"],\n",
    "        \"layer2.weight\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv2.weight\"],\n",
    "        \"layer2.bias\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv2.bias\"],\n",
    "        \"layer3.weight\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv3.weight\"].repeat(3, 1, 1, 1),\n",
    "        \"layer3.bias\": torch.load(\"../models/Git/SRCNN.pth\")[\"conv3.bias\"].repeat(3),\n",
    "    }\n",
    "    srcnn.load_state_dict(adjusted_state_dict)\n",
    "\n",
    "    # Mover al dispositivo y configurar en modo evaluación\n",
    "    srcnn.to(device)\n",
    "    srcnn.eval()\n",
    "\n",
    "    # Procesar el dataset\n",
    "    process_npz_dataset(dataset_path, output_dir, srcnn, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
